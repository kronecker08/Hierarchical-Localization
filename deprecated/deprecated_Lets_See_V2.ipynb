{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fleet-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSS\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "from hfnet.models import get_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from hloc.utils.base_model import dynamic_load\n",
    "from hloc import extractors\n",
    "from hloc.utils.tools import map_tensor\n",
    "# import Mask_RCNN_class\n",
    "feature_config ={'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 3},'preprocessing': {'grayscale': True, 'resize_max': 960}}\n",
    "\n",
    "global_config = {'data': {'name': 'aachen', 'load_db': False, 'load_queries': True, 'resize_max': 960}, 'model': {'name': 'netvlad_original', 'local_descriptor_layer': 'conv3_3', 'image_channels': 1}, 'weights': 'vd16_pitts30k_conv5_3_vlad_preL2_intra_white/vd16_pitts30k_conv5_3_vlad_preL2_intra_white'}\n",
    "path_for_weights_netVLAD = \"/home/Hierarchical-Localization/weights/vd16_pitts30k_conv5_3_vlad_preL2_intra_white/vd16_pitts30k_conv5_3_vlad_preL2_intra_white\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,4'\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adapted-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    feature_config ={'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 3},'preprocessing': {'grayscale': True, 'resize_max': 960}}\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.load_superpoint()\n",
    "#         self.load_global_descriptor()\n",
    "#         self._mask_rcnn = Mask_RCNN_class.Model_RCNN()\n",
    "        \n",
    "    def new_image(self,image, use_Mask_RCNN=False):\n",
    "        self.image = image ## send in BRG instead of RGB \n",
    "        \n",
    "        if (use_Mask_RCNN):\n",
    "            self.masking()\n",
    "        self.processed()\n",
    "        self.key_point_and_descriptor()\n",
    "        \n",
    "          \n",
    "    def masking(self):\n",
    "        self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB)\n",
    "        self.image =self._mask_rcnn .rcnn_results_func(self.image)\n",
    "        return self.image\n",
    "\n",
    "    def processed(self):\n",
    "        if feature_config[\"preprocessing\"][\"grayscale\"]:\n",
    "            image_processed = cv2.cvtColor(self.image,cv2.COLOR_RGB2GRAY)\n",
    "        if image_processed is None:\n",
    "            raise ValueError(f'Cannot read image {str(path)}.')\n",
    "        image_processed = image_processed.astype(np.float32)\n",
    "        size = image_processed.shape[:2][::-1]\n",
    "        w, h = size\n",
    "\n",
    "        if feature_config[\"preprocessing\"][\"resize_max\"] and max(w, h) > feature_config[\"preprocessing\"][\"resize_max\"]:\n",
    "            scale = feature_config[\"preprocessing\"][\"resize_max\"]  / max(h, w)\n",
    "            h_new, w_new = int(round(h*scale)), int(round(w*scale))\n",
    "            image = cv2.resize(\n",
    "                image_processed, (w_new, h_new), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if feature_config[\"preprocessing\"][\"grayscale\"]:\n",
    "            image_processed = image_processed[None]\n",
    "        else:\n",
    "            image_processed = image_processed.transpose((2, 0, 1))  # HxWxC to CxHxW\n",
    "        image_processed = image_processed / 255.\n",
    "\n",
    "        self.data = {\n",
    "            'image': (torch.from_numpy(image_processed)).unsqueeze(0),\n",
    "            'original_size': torch.from_numpy(np.array([size])),\n",
    "        }\n",
    "        return self.data\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def load_superpoint(self):\n",
    "        self.device_sp = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         with torch.cuda.device(0):\n",
    "        print(\"Device for superpoint\", self.device_sp)\n",
    "        Model_sp = dynamic_load(extractors, feature_config['model']['name'])\n",
    "        self.model_sp = Model_sp(feature_config['model']).eval().to(self.device_sp)\n",
    "\n",
    "    def key_point_and_descriptor(self):\n",
    "        pred_sp = self.model_sp(map_tensor(self.data, lambda x: x.to(self.device_sp)))\n",
    "        pred_sp = {k: v[0].cpu().numpy() for k, v in pred_sp.items()}\n",
    "        pred_sp['image_size'] = original_size = self.data['original_size'][0].numpy()\n",
    "        if 'keypoints' in pred_sp:\n",
    "            size = np.array(self.data['image'].shape[-2:][::-1])\n",
    "            scales = (original_size / size).astype(np.float32)\n",
    "            pred_sp['keypoints'] = (pred_sp['keypoints'] + .5) * scales[None] - .5\n",
    "#         if as_half:\n",
    "#             for k in pred:\n",
    "#                 dt = pred[k].dtype\n",
    "#                 if (dt == np.float32) and (dt != np.float16):\n",
    "#                     pred[k] = pred[k].astype(np.float16)\n",
    "# #         name = data['name'][0] ## name of the file\n",
    "        pred_sp[\"name\"] = \"query\"\n",
    "        \n",
    "\n",
    "        logging.info('Finished exporting features.')\n",
    "        self.pred_copy = pred_sp \n",
    "        del pred ## do not know why they are doing this \n",
    "        return self.pred_copy\n",
    "    def load_global_descriptor(self):\n",
    "        #with tf.device('/job:localhost/replica:0/task:0/device:XLA_GPU:2'):\n",
    "        self.net = get_model(global_config['model']['name'])(data_shape={'image': [None, None, None, 3]},**global_config['model'])\n",
    "        checkpoint_path = Path(path_for_weights_netVLAD)\n",
    "        self.net.load(str(checkpoint_path))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "willing-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for superpoint cuda\n",
      "Loaded SuperPoint model\n"
     ]
    }
   ],
   "source": [
    "q = Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lesbian-january",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5c31f64bd0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_array_for_feature_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/Hierarchical-Localization/datasets/Images_query_folder/iPhoSilWithWv4_1598593698_648-02165.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array_for_feature_detector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# i = q.masking()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7f66f189a9d3>\u001b[0m in \u001b[0;36mnew_image\u001b[0;34m(self, image, use_Mask_RCNN)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_point_and_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7f66f189a9d3>\u001b[0m in \u001b[0;36mkey_point_and_descriptor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkey_point_and_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpred_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_sp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mpred_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_sp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mpred_sp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'keypoints'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_sp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7f66f189a9d3>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkey_point_and_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpred_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_sp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mpred_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_sp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mpred_sp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'keypoints'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_sp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "image_array_for_feature_detector = cv2.imread(\"/home/Hierarchical-Localization/datasets/Images_query_folder/iPhoSilWithWv4_1598593698_648-02165.png\")\n",
    "\n",
    "q.new_image(image_array_for_feature_detector)\n",
    "# i = q.masking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "_, ax = plt.subplots(1, figsize=(16,16))\n",
    "ax.imshow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_global_descriptor(self):\n",
    "        with tf.device('/device:GPU:2'):\n",
    "            self.net = get_model(global_config['model']['name'])(data_shape={'image': [None, None, None, 3]},**global_config['model'])\n",
    "            checkpoint_path = Path(path_for_weights_netVLAD)\n",
    "            self.net.load(str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-volume",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
